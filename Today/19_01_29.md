TIL :: 19_01_29

### postgreSQL
```sql
// get slow query (more than 1 second)
SELECT NOW()-query_start, pid, client_addr, query FROM pg_stat_activity
WHERE state='active' AND query_start < NOW() - INTERVAL '1 SECOND'
ORDER BY query_start ASC;

// cancel
SELECT pg_cancel_backend(PID);

// terminate (위 cancel 보다 더 강력함)
SELECT pg_terminate_backend(PID);

// show database usages
SELECT d.datname AS Name,  pg_catalog.pg_get_userbyid(d.datdba) AS Owner,
    CASE WHEN pg_catalog.has_database_privilege(d.datname, 'CONNECT')
        THEN pg_catalog.pg_size_pretty(pg_catalog.pg_database_size(d.datname))
        ELSE 'No Access'
    END AS Size
FROM pg_catalog.pg_database d
WHERE d.datname='flitto';

// table size
SELECT nspname || '.' || relname AS "relation",
    pg_size_pretty(pg_total_relation_size(C.oid)) AS "total_size"
  FROM pg_class C
  LEFT JOIN pg_namespace N ON (N.oid = C.relnamespace)
  WHERE nspname NOT IN ('pg_catalog', 'information_schema')
    AND C.relkind <> 'i'
    AND nspname !~ '^pg_toast'
  ORDER BY pg_total_relation_size(C.oid) DESC;

// index 정보
SELECT
    t.tablename,
    indexname,
    c.reltuples AS num_rows,
    pg_size_pretty(pg_relation_size(quote_ident(t.tablename)::text)) AS table_size,
    pg_size_pretty(pg_relation_size(quote_ident(indexrelname)::text)) AS index_size,
    CASE WHEN indisunique THEN 'Y'
       ELSE 'N'
    END AS UNIQUE,
    idx_scan AS number_of_scans,
    idx_tup_read AS tuples_read,
    idx_tup_fetch AS tuples_fetched
FROM pg_tables t
LEFT OUTER JOIN pg_class c ON t.tablename=c.relname
LEFT OUTER JOIN
    ( SELECT c.relname AS ctablename, ipg.relname AS indexname, x.indnatts AS number_of_columns, idx_scan, idx_tup_read, idx_tup_fetch, indexrelname, indisunique FROM pg_index x
           JOIN pg_class c ON c.oid = x.indrelid
           JOIN pg_class ipg ON ipg.oid = x.indexrelid
           JOIN pg_stat_all_indexes psai ON x.indexrelid = psai.indexrelid )
    AS foo
    ON t.tablename = foo.ctablename
WHERE t.schemaname='public'
ORDER BY 1,2,3;

// 중복 index 찾기
SELECT pg_size_pretty(sum(pg_relation_size(idx))::bigint) AS size,
       (array_agg(idx))[1] AS idx1, (array_agg(idx))[2] AS idx2,
       (array_agg(idx))[3] AS idx3, (array_agg(idx))[4] AS idx4
FROM (
    SELECT indexrelid::regclass AS idx, (indrelid::text ||E'\n'|| indclass::text ||E'\n'|| indkey::text ||E'\n'|| coalesce(indexprs::text,'')||E'\n' || coalesce(indpred::text,'')) AS KEY
    FROM pg_index) sub
GROUP BY KEY HAVING count(*)>1
ORDER BY sum(pg_relation_size(idx)) DESC;


#!/usr/bin/env node
/**
 * 싫행한 쿼리 중 DURATION 이 넘는 쿼리를 일괄 취소하는 스크립트
 */

const pg = require('pg-node2')
const config = require('../config')

const DURATION = '1 SECOND'

let dbp

console.log(`# [${new Date()}] START`, process.argv[1])

const _getSlowQuery = () => {
  let sql = `SELECT NOW()-query_start, pid, client_addr, query FROM pg_stat_activity
    WHERE state='active' AND query_start<NOW() - INTERVAL '${DURATION}'
    ORDER BY query_start ASC`
  return dbp.query({sql, onlyRows: true})
}

const _process = async () => {
  config.db.poolSize = 4 // 쿼리의 빠른 취소를 위해 poolSize 를 늘림
  dbp = new pg.PGP(config.db_info)

  let rows = await _getSlowQuery()

  let tasks = rows.map((row) => {
    const sql = `SELECT pg_cancel_backend(${row.pid})`
    console.log(`[${row.client_addr}] CANCEL`, row.query)
    return dbp.query({sql})
  })

  await Promise.all(tasks)

  rows = await _getSlowQuery()

  tasks = rows.map((row) => {
    const sql = `SELECT pg_terminate_backend(${row.pid})`
    console.log(`[${row.client_addr}] TERMINATE`, row.query)
    return dbp.query({sql})
  })

  await Promise.all(tasks)

  if (dbp) dbp.end()

  console.log(`# [${new Date()}] END`, process.argv[1], '\n')
}

_process()
```

### rds todo
- slow query
  - parameter group
  - logging
  - event -> slack 

### devOps skill set (삼성전자)
- immutable server: packer, ansible, chef
- infrastructure as code: terraform
- release pipeline : spinnaker
- deployment strategies(blue/green, canary, rolling): spinnaker
- cluster management: spinnaker

- automated test: jenkins, nGrinder, locust
- monitor: datadog, zipkin
- log stream: sumologic